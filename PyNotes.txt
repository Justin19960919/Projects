
Command+Shift+G  (search pc by route)
Command+Shift+G (open package control in st3)
which python (open cmd, check python route in pc)
conda list (checking conda's installed packages)

## checking python version
python --version
# checking pathway to python
which pyhthon

# 安裝 requests, lxml, pandas
pip install -U requests lxml pandas

# upgrading packages
pip install --upgrade numpy==1.19.1
1. To install the latest version of a package: >>pip install 'PackageName'
2. To install a specific version, type the package name followed by the required version: >>pip install 'PackageName==1.4'
3. To upgrade an already installed package to the latest from PyPI: >>pip install --upgrade PackageName.

## Launching Jupyter notebook
- open terminal
jupyter notebook

- shut program
Ctrl+C : shut down kernel (y/n)

####################################### Decorators #############################

Meta-programming: A part of the program modifies another part at compile time







####################################### Conditions #############################
Inline if else statement:
x = a if b else 0




################################################################################

---------------------------------
# functions
輸入 pip3 install libraries

Def ____ (參數=＿＿):
	
依照傳入順序 / call by parameter

def borrowing(item, ndays):
	print("Please return %s in %d days." %(item, ndays))

borrowing("books", 30) #Please return books in 30 days.
borrowing(ndays = 25, item = "books") #Please return books in 30 days.

靠global變數取得在function裡面的值
---------------------------------

Mutable: dict / list/ set/

---------------------------------
## Math functions

import math

math.e
math.exp()
math.factorial()
math.ceil()
math.floor()
math.pow()

####################################### String ###################################
# String "" ''

str1='abcd'
str[-1]-->d

slicing:
[start:end] end 只會到指定index的前一個 ; end-1
[start:end:間距] 每隔幾個間距 摘錄
[start:]
[:end]

+ concatenation
* repetition


* Reverse a string
string[::-1] # first two : means slicing

#ex:
msg= “This is a string”
msg[-1:-7:-1] # “gnirts”
msg[6:4:-1] #“si”


## enumerate(string):
可以幫你瞬間造好index，裡面各個元素是tuple
>>>seasons = ['Spring', 'Summer', 'Fall', 'Winter']
>>> list(enumerate(seasons))
[(0, 'Spring'), (1, 'Summer'), (2, 'Fall'), (3, 'Winter')]


ASC11
65-90 A-Z
97-122 a-z
48-57 0-9

ord(): 傳進去一個character的字串，就會回傳ASC編碼

## 字串處理 

.capitalize() 大寫字串第一個字
.title() 每個字第一個字都大寫
.upper()
.lower()
.replace(,)
in 
.count() 計算某個字符在該字串出現的頻率個數
.find() 最小index裡面，這個東東在哪裡出現
.isupper() ??是不是都是大寫
.split() 分割結果存成list
.strip("____") 只能刪除頭尾的字符
.join() 串接list中的所有字串  EX: "某個符號".join(string_list)


## 字串的格式輸出
%<width>.<precision><type-char>

<width> 總共show幾個出來，0表示要多少給多少
<precision> 幫你留幾位小數
<type-char> decimal(base 10 ints) ; float ; string    d/ f/ s
EX: %-10.3f  -> '-'就是向左對齊

格式化輸出(前面補東西)：
print( '%05.1f' % number )  包括小數點總共5位，小數點後1位，不足補0

'_%s...____%d' %(___,___) ## ():tuple


## format
print("Testing pandas {}".format("functions"))
"{.:2f}".format(0.2342)  ## only output two decimal places


** using the f string to format (faster)
print(f'{_can fit in variable_} and continue it write strings in here')
EX:
name = 'Tushar'
age = 23
print(f"Hello, My name is {name} and I'm {age} years old.") 
print(f'score : {cross_val_score(estimator, data, train_Y, cv=5).mean()}')

###################################   loops    ###############################


range(start, end,step_size)
range(1,10,1) --> 1~9
range(0,20,5) --> 0 5 10 15
# reverse range
range(20,0,-5) --> 20,15,10,5   ## stops at the previous item from second arg of range()
range(20,10,-1) --> 20,19,.....11


for index, item in enumerate(list or string):
	...




#################################################################################

# Data Structure /read files

tuple
dic
set 
datetime

** sorted()會保留原本的，產生另外一個sorted的物件


--------  list / arrays (ordered and mutable) -------- 
## methods:

.insert(some_index) # insert at specific index
.pop(some_index)  # remove item based on index ; .pop() removes last index
,remove(item) # remove item from list
.reverse()
.count(some_item) # returns count of a specified value
.copy()
.clear()




## sorting
list.sort()
List.sort(reverse=True)
- sorting dictionary
def myFunc(e):
  return e['year']

cars = [
  {'car': 'Ford', 'year': 2005},
  {'car': 'Mitsubishi', 'year': 2000},
  {'car': 'BMW', 'year': 2019},
  {'car': 'VW', 'year': 2011}
]

cars.sort(key=myFunc)

# A function that returns the length of the value:
def myFunc(e):
  return len(e)

cars = ['Ford', 'Mitsubishi', 'BMW', 'VW']
cars.sort()	// In ascending order
cars.sort(reverse=True, key=myFunc)	// In descending order



--------  tuples: (ordered & immutable)   --------
tuple()
(,)
Tuple 可以相加 tuple1+tuple2
slice: []
當新增時，就會新切一個記憶體
tuple只能串tuple
複製tuple，是複製他的值

sorted(tuple)會變成list，並且舊的tuple不動
-----
zip()  #將對應元素打包成為tuple, 成為一組。

a = [1,2,3]
b = [4,5,6]
zipped = zip(a,b)     # 打包为元组的列表
[(1, 4), (2, 5), (3, 6)]

-----
# lambda

ex:
f2=lambda x:x**2
#多個引數的：
g = lambda x, y, z : (x + y) ** z
print g(1,2,2)


# map
對list中的每一個元素都做一樣的事情，就不需要for去跑
out2=map(lambda x:x**2, list1)





-------  dictionary : dict() Unordered, mutable  -------- 
》》is mutable; can use key to change value
{key:value}	ex: a = dict(one=1, two=2, three=3)
dictionary['key']


Create dictionary:
dict()
{}


pizza_prices = {
  “Pepperoni” :  8.99,
  “Hawaiian”: 12.99,
   “Veggie supreme”: 11.99
}






dictionary.keys()  or list(some_dictionary)	## get all the keys	
dictionary.values()  ## return values of a dictionary
dictionary.items() 	## print keys, values 

for a,b in dictionary.items():

for k in dict:
	print(k) # iterates over the keys

* dict methods *

# Add items
dict[key]=value

# check if x is in the dictionary keys
x in dict 	

# deletion
1. del dict['somekey']
2. dict.pop('somekey')

# copy
somedict.copy()





-------  set {} Unordered, mutable  -------- 
set()
1.轉成set 後只能元素只會出現一次
2. 也不能用+，就是不允許串接的意思，只能做集合運算！


# To access use iterators:
for item in set:

# Add items

- add 1 item
_some set_.add(__a item__)

- add multiple items
_some set_.update([a list of items])



# use bitwise logical operators
> union: | 
a set|{55}
> intersect: &   EX:A&B	set.intersection(other_set)
> difference: 差集 a-b(把在a集合裡面b集合的東西扣掉)
> Symmetric difference : 全部 減掉 共同有的  EX:A^B

################################# Datetime #################################
## Datetime
import datetime

name=datetime.datetime() ex:yy-mm-dd hr-min-s  
EX: datetime.datetime(2017,8,6,5,20,30)  >>> 2018-08-06 05:20:30

name.year
name.month
name.day
name.hour
name.minute
name.second


name.date() #用在datetime.datetime()之後，可以從整個datetime得到只有date
name.time() # 同理，只得到time
name.days
name.seconds

name.date().weekday()那天是禮拜幾  # monday~sunday =0~6
datetime.date().today() #回傳一個date 的 object

.timedelta(days=__,seconds=___,microseconds, milliseconds, minutes, hours, weeks)  # 可以進行時間上的相加


## datetime to string
name=datetime.datetime(......)
name.strftime('%Y-%m-%d') # 年月日  
## 這裡面可以放很多變化
EX: print(d7.strftime('%Y-%m-%d %I:%M:%S %p, %A')) #2002-05-02 01:15:45 PM, Thursday



## String to datetime
name="___"
___=datetime.datetime.strptime(name,表示方式)  
'%Y-%m-%d %H:%M:%S' # H表示24小時制



################################# File Handling #################################

# 3-7 檔案讀寫 (read files)
<filevar>=open(<name>,<mode>,encoding=<encoding>)
## open function takes two parameters; filename, and mode.
"r" - Read - Default value. Opens a file for reading, error if the file does not exist
"a" - Append - Opens a file for appending, creates the file if it does not exist
"w" - Write - Opens a file for writing, and overrides the whole file
"x" - Create - Creates the specified file, returns an error if the file exists

rt   ## default text
rb<->wb   #binary file : non text file (Binary mode: ex:images)
r<->w  # text file
 
EX: 
infile=open("numbers.dat","rb")

path 使用 \\ 代替 \


############## reading files ############## 

## read in file
f = open("demofile.txt", "r")  ## f is an array we can loop across

# read in
f.read()	# reads in whole file
f.read(5)	# reads 5 lines
f.readlines()	# reads in as a list of strings


## read line by line
file = open(...)
lines = []
for line in file:
	lines.append(line)
file.close()

## best practice
lines = []
with open(filepath, "r") as file:
	for line in file:
		lines.append(line)


# close file
f.close()




############## writing files ############## 

## write to existing file



f = open("demofile.txt", "a") # means append
f.write("Now the file has more content!")
f.close()


# creates a file if it doesn't exist, overwrites if it does
f = open("__.txt","w")t
f.write("some text")
f.close()

f.writelines(__some list of strings__)

file = open(file_path, "w")
for line in lines:
	file.write(line + "\n")
file.close()

# best practice

def best_practice(file_path):
    '''
        Function -- best_practice
            Demonstrates how to use "with" to automatically close the file.
        Parameter:
            file_path -- the path of the file
        Returns:
            Nothing.
    '''
    lines = generate_lines()
    # Opens the file and assigns it to the variable called file
    with open(file_path, "w") as file:
        # The following is the line by line option but the others work too
        for line in lines:
            file.write(line + "\n")
    # The file is automatically closed when using "with




##Check if File exist
import os
if os.path.exists("demofile.txt"):
  os.remove("demofile.txt")
else:
  print("The file does not exist")

## Using with to read files

with open("/Users/justin/Desktop/ForFun/obama_p1.txt", "r") as ob:
	obb=ob.read().lower()

print(obb)   >> this outputs the whole textfile
print(type(obb))  >> str



################################# Exception handling #################################

isinstance(object,type) #ex: isinstance("pie",str)

--- Raising Errors ---
if _____:
	raise ValueError("____")
	raise TypeError("____")



-----  Exception Handling -------
** Try-Except-Raise **

try:
#先執行這裡

except 某一種error  :
# 如果有error就會跳到這裡; you can print out something ; but doesn't print error (If there is a loop, then except handles it)


finally:
不管try except結果如何一定會執行finally這裏


Ex:
except TypeError as ex:
	print(ex)

except ValueError as ex:
	print(ex)

• ValueError: some value is an incompatible type 
• SyntaxError: typos
• IndexError: nothing exists at the provided index 
• RecursionError: forgot the base case


### Raising Errors ###
屬於error的部分，也不用寫break<
When an exception is raised, any code that follows will not execute
raise Exception("   ")

# we can raise exceptions by ourselves
raise ___a built in exception type ___
raise ValueError("________")

Ex:

def time_validator(time):
  parts = time.split(“:”)
  if len(parts) != 2:
    raise ValueError(“Time must be in the format HH:MM”)



- except ValueError:
	raise ValueError("___")


** Unit testing with exceptions
from pytest import raises
with raises(__some error__):
	some_function(" your test case")


################################# Matplotlib #################################
Python Graph Gallery:
https://python-graph-gallery.com


import matplotlib.pyplot as plt
From matplotlib.font_manager import FontProperties

## 中文編碼有問題，須先指定
ChineseFont2 = FontProperties(fname="C:/Windows/Fonts/mingliu.ttc")  ## Mac有自己的設定

fig, ax= plt.subplots()

ax.scatter(betalist, alphalist)

#把每支股票的名字點在圖上
For I,txt in enumerate(namelist):
	ax.annotate(txt,(betalist[i],alphalist[i]),ontproperties= ChineseFont2)
plt.show()

## plt.plot
import matplotlib.pyplot as plt
years = [1950,1960,1965,1970,1975,1980,
        1985,1990,1995,2000,2005,
        2010,2015]
pops = [2.5,2.7,3,3.3,3.6,4.0,
        4.4,4.8,5.3,6.1,6.5,6.9,7.3]
deaths = [1.2,1.7,1.8,2.2,2.5,2.7,2.9,3,3.1,3.2,3.5,3.6,4]
lines = plt.plot(years,pops,years,deaths)  ## 這個可以一次畫兩條線

#plt.plot(years,pops,'k-')

## add another line 就再plot一次就好
#plt.plot(x,y,'k-',color=?)
l,y=lines
print(l)
print(y)
#color=('#7c76bc','#4376bc')
plt.setp(lines,marker = "o",linewidth=2,linestyle=':') ## linestyle ; linewidth ; coloe 
plt.grid(True)
plt.title("Population Growth") # title
plt.ylabel("Population in billions") # y label
plt.xlabel("Population growth by year") # x label
plt.show()


## Matplot data visulization


# 設定繪圖區域的長與寬
plt.figure(figsize = (10, 8))



- histogram example
(In this case, for both teams, so it can be compared)
d = pd.read_csv('runs.csv')
score_india = d['score_india']
legend = ['India', 'Pakistan']
score_pk = d['score_pk']
plt.hist([score_india, score_pk], color=['orange', 'green'])
plt.xlabel("Runs/Delivery")
plt.ylabel("Frequency")
plt.legend(legend)
plt.xticks(range(0, 7))
plt.yticks(range(1, 20))
plt.title('Champions Trophy 2017 Final\n Runs scored in 3 overs')

plt.savefig('test_plot.png', bbox_inches='tight') ## saving the plot auto

plt.show()


## scatterplot
https://jakevdp.github.io/PythonDataScienceHandbook/04.02-simple-scatter-plots.html
https://plot.ly/matplotlib/line-and-scatter/

plt.scatter(pltt['time'],pltt['Views'],alpha=0.5) ## alpha?  ; line widths
plt.title('Xvideos scatter plot')
plt.xlabel('Time')
plt.ylabel('Views')
plt.show()

## histogram
plt.hist(pltt['time'],bins='auto',align='mid',rwidth=0.85,color='#0504aa')
plt.title('Xvideos time histogram')
plt.xlabel('Time')
plt.ylabel('Freq')
plt.xticks(range(0, 8000,400))
plt.show()

## piechart
Labels
Size  ## 內容
plt.pie(size , labels = labels,autopct='%1.1f%%'.explode=__) 
#autopct='%1.1f%%'是用來顯示百分比。
#explode 後面可以填tuple或list，會讓piechart其中一個跑出來，要跟size維度相等
plt.axis('equal')
plt.show()

## bar chart
https://ithelp.ithome.com.tw/articles/10196410



-------------------- Introductionn to data visualization with Matplotlib (DataCamp) ----------------
fig, ax = plt.subplots()

fig, ax = plt.subplots(3,2)		// 3 rows, 2 columns

# setting the width and height of the plot
fig.set_size_inches([__,__])	 // width, height


# index into the object
ax[0,0].plot(x,y)	# plot for row0, col 0

# Small multiples (Plotting multiple subplots on a canvas)
Ex:
fig, ax = plt.subplots(2,1,sharey = True)	# share the y axis grid
ax[0].plot(..)
ax[1].plot(..)





# Plot MLY-PRCP-NORMAL from seattle_weather against the MONTH
ax.plot(seattle_weather["MONTH"], seattle_weather["MLY-PRCP-NORMAL"],marker="o")
- marker = 'o' adding circles as data points

	='v'  downward triangles as data points
- linestyle = "--"	 lifestyle is now --
- color = "r"	color is now red


ax.set_xlabel("")
ax.set_ylabel("")
ax.set_title("")

plt.show()



# Parse the date column as dates and use data column as index

climate_change = pd.read_csv('climate_change.csv', parse_dates=["date"], index_col="date")

# slicing with pandas data frame indexes (dates)
seventies = climate_change["1970-01-01":"1979-12-31"]

# Using two y axes (twin axes)
ax2 = ax.twinx()
ax2.plot(x,y,color="")
ax2.set_ylabel(color="")	//setting the ylabel to be __ color
ax2.tick_params('y',colors="")		// coloring the color of the ticks (The first parameter can be 'x' or 'y')

# adding annotations (pieces of text that explain a feature)
ax.annotate("text to add", xy = [x's position, y's position])
ax2.annotate(">1 degree", xy=(pd.Timestamp('2015-10-06'), 1),xytext=(pd.Timestamp('2008-10-06'), -0.2),arrowprops={"arrowstyle":"->", "color":"gray"})

# adding arrow to the annotation
arrowprops = {}	//default property
arrowprops={"arrowstyle":"->", "color":"gray"}	# thin arrow with widehead



------ Quantitive comparisons and statistical visualizations ------- 

#### Barcharts (ax.bar()) #### 

ax.bar(x,y)
ax.set_xticklabels(x,rotation = 90)	//rotation by 90 deg
ax.set_ylabel("   ")

# stacked bar chart
ax.bar(x,y,label = "")
	- x
	- y
	-label
	-yerr = __some column__.std()


ax.bar(x,z,bottom=y,label = "") stack z onto y
ax.bar(x,g,bottom=y+z,label = "") stack g onto y+z
ax.legend()	// shows the legends, the labels specified above
plt.show()

## Statistical barplots
ax.bar("Rowing", mens_rowing['Height'].mean(), yerr=mens_rowing['Height'].std())



#### Histograms #### 

ax.hist(x,y,bins = ___)
ax.hist(x,y,....) 	//calling multiple ax.hists() plot multiple histograms on the same canvas

- Parameters:
bins = __    
	default:10 
	bins = [10,20,30..] 	// we can also specify boundaries
histtype
	default: "bar"
	"step" : This only draw the outline of the histogram



#### Boxplots #### 
ax.boxplot([x1,x2]])	//plotting  two box plots side by side
ax.set_xticklabels([x1_name, x2_name])
ax.set_ylabel(y column name)
plt.show()


# Adding error bars to plots (Line plot with error bars)
ax.errorbar(x,y,yerr = )


#### Scatter plot ####
ax.scatter(x,y)
	- x
	- y
	- c = ___.index  # colors the scatter dots with a discrete variable


------ Alternate the style of the plot ------- 
plt.style.use()
Fill in:
	"ggplot"
	"bmh"
	"seaborn-colorblind"	//seaborn is for statistical visualization
	"tableau-colorblind10"
	"grayscale"


----- Sharing and saving the visualizations ------- 
fig.savefig("____.png")
	.png		// loss-less compression
	.jpg		// Used in websites
	.svg 		// Useful for further editing	

quality 	// 1-100
dpi		// resolution






################################# Class #################################
# Class name should start with a capital letter, file should be lower case
EX: class Car ; save in car.py

# Class 自訂類別、自訂資料型態
attribute: variable attached to the class
method: function attached to the class

Declare an object by calling a  class
_instance_ = _some_class_()  #creating an instance by calling the class

Constructor: A function that created an instance of the class  #ㄧ定要被呼叫，
__init__()

Ex:
class Date:
	def __init__(self,year="1996",month,day):
	## you can make default values too EX:1996, now year is an optional parameter
		self.year = year  
		self.month = month
		self.day = day

# if year is optional:
d1 = Date(year="2019",2,3)

d = Date(2018,4,5) # creating


***** Special Methods in classes *****

1.
__str__():

def __str__(self):
	output = "The date is " + self.year+" "+ self.month  # this is the string we want to print
	return output

Ex:
(This method prints the instance d)	//just directly print the object out
print(d) # This prints: "The date is 2018 4"
 

## If one class calls __str__ to print out another interacting object, then the other class needs to have the __str__ method

2.
__eq__():
Comparison between an object and another
def __eq__(self,other):
	return .... True if self == other, False if not


See if two objects are equal
Ex:
    def __eq__(self,other):
        # we also need to checl self and other are the same type, eg. the same class first
        if type(self) != type(other):
            return False
        status = self.month == other.month and self.day == other.day and self.year == other.year
        return status

3.
__lt__():
def __lt__(self,other):

__gt__():
obj1 > obj2	
	

4. __add__():
What happens when u add objects? obj1 + obj2

################################# class method vs static method ######################

****   static method就是包含在這個class裡面的正常的method，像是函式包裡面一個預先建好的method。
@staticmethod 
Def function(param1):
	parameters 幹嘛幹嘛
	return ..

Call function: 
你的class.該static method


****  class methods  :receive class as first argument rather than instance
@classmethod
Def function1(cls,param1):  ## cls 必須是第一個argument
  cls.variable(前面已經定義好的了)=param1


myclass.function1(____) ＃使用class的方式來set values
相當等於

class.variable=_____

## accessing attributes

Instead of using the normal statements to access attributes, you can use the following functions −

The getattr(obj, name[, default]) − to access the attribute of object.

The hasattr(obj,name) − to check if an attribute exists or not.

The setattr(obj,name,value) − to set an attribute. If attribute does not exist, then it would be created.

The delattr(obj, name) − to delete an attribute.

## Class inheritance
Class sub_class(original_class):
	pass
直接inherit original_class的所有method ; 
subclass：裡面可以change一些設定好的變數，但不會破壞原本的original class

issubclass():
# built-in function : 判斷是不是subclass

** Multiple inheritance
A subclass inherits all the variables and methods from more than one parents

>>> class Parent1:
...     x = 1
... 
>>> class Parent2:
...     y = 1
... 
>>> class Parent3:
...     z = 1
... 
>>> class Kid1(Parent1, Parent2, Parent3):
...     u = 2
...


Kid1 inherited all three classes in Parent, Parent2, and Parent3

################################# UNit testing ######################

assertAlmostEqual(): used in floats


from car import Car  ##Car is another class
import unittest

class CrashTest(unittest.TestCase):
    def test_init(self):
        car = Car("Herbie", "Lovebug", price = 400.0)
        self.assertEqual(car.make, "Herbie")
        self.assertEqual(car.model, "Lovebug")
        self.assertEqual(car.year, 2019)
        self.assertAlmostEqual(car.price, 400.0)

    def test_add_feature(self):
        car = Car("Herbie", "Lovebug", price = 100.0)
        car.add_feature("heated seats")
        self.assertAlmostEqual(car.price, 300.0)

        car.add_feature("power steering")
        self.assertAlmostEqual(car.price, 650.0)


def main():
    unittest.main(verbosity = 3)  
    # calling unittest.main() automatically tests all functions
    # verbosity = 3, gives more detail into what functions are tested

main()


*** For reference : Car class
    def __init__(self, make, mod, year=2019, price=25000.0):
        '''
        Constructor -- creates an new instance of a car
        Parameters:
           self -- the current object
           make -- the initial make of this car
           model -- the initial model of this car
           year (optional) -- the initial year of this car
           price (optional) -- the initial price of this car
        '''
        self.make = make
        self.model = mod
        self.year = year
        self.price = price

    def add_feature(self, item):
        '''
        Method -- add a feature to this car
        Parameters:
           self -- the current object
           item -- the feature to add to this car
        Returns nothing
        '''
        if item == "heated seats":
            self.price += 200
        elif item == "power steering":
            self.price += 350


################################# statistics #################################
import statistics as st
mean()  #Arithmetic mean (“average”) of data.
harmonic_mean() #Harmonic mean of data.
median() #Median (middle value) of data.
median_low() #Low median of data.
median_high() #High median of data.
median_grouped() #Median, or 50th percentile, of grouped data.
mode() #Mode (most common value) of discrete data.

pstdev()  #Population standard deviation of data.
pvariance() #Population variance of data.
stdev() #Sample standard deviation of data.
variance() #Sample variance of data.



# Two sample t-tests
scipy.stats.ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate')






################################# os #################################
匯入os
import os
1.更改文件檔名 os.rename
os.rename(“list.txt”,”123.txt”)
os.rename(原檔名, 新檔名)

2.刪除檔案 os.remove
os.remove(“list.txt”)

3.建立資料夾 os.mkdir
os.mkdir(“資料夾名稱”)
#檢查目錄是否存在，若無，則建立資料夾
import os
dir='myDir'
if not os.path.exists(dir):
    os.mkdir(dir)
else:
    print(dir + '已經建立！')

4.獲取當前目錄 os.getcwd()
os.getcwd()

5.返回上層目錄 os.chdir(“../”)
os.chdir(“../”)


6.獲取目錄列表 os.listdir(“./”)
os.listdir(“./”)

7.刪除資料夾 os.rmdir( )
os.rmdir(“資料夾名稱")

8.批量修改檔名
import os
file_name = “txt文件”
file_names = os.listdir(file_name)
os.chdir(file_name)
for name in file_names :
os.rename(name,"要添加的文字"+ name)

9.遞迴列出所有資料夾與檔案
import os
path='f:\\python'
def find_dir(dir):
    fds=os.listdir(dir)
    for fd in fds:
        full_path=os.path.join(dir,fd)
        if os.path.isdir(full_path):
            print('資料夾:',full_path)
            find_dir(full_path)
        else:
            print('檔案:',full_path)
find_dir(path)

10.列出指定結尾檔案
import os
path='f:\\python'
for root,dirs,files in os.walk(path):
    for file in files:
        if file.endswith('.py'):             #py為結尾
            print(os.path.join(root,file))

11.執行作業系統命令
import os
cur_path=os.path.dirname(__file__) #取得目前路徑
os.system('cls') #清除螢幕
os.system('mkdir dir2') #建立dir2目錄
os.system('copy ossystem.py dir2\copyfile.py') #複製檔案
file=cur_path +'\dir2\copyfile.py)
os.system('notepad' + file) #以記事本開啟copyfile.py檔案

11.使用os.walk列出所有JPG與PNG檔案
import fnmatch, os
path='f:\\python'
exts = ['*.jpg','*.jpeg','*.png']
mathches = []
for root, dirs, files in os.walk(path):
    for ext in exts:
        for file in fnmatch.filter(files,ext):
            matches.append(os.path.join(root, file))
for image in matches:
    print(image)

12.找出高頻率出現單字
from collections import Counter
import re
fin=open('zen.txt','rt')
s=fin.read().lower()
words=re.findall(r'[\w\']+',s)
c=Counter(words)
print(c.most_common(5))
File檔案
開檔案
f = open(file,mode,encoding='utf-8')
#file:路徑
#mode:'r','w','a'
#r:讀取模式(預設)
#w:寫入模式(清除原本內容)
#a:附加模式(附加在檔尾)
方法
f.close() #執行完後關檔
f.read()  #讀取所有字元
免用close()方法的讀檔方式
with open (....) as f:
    ....
    程式區塊
    ....
檢查網頁是否更新
import hashlib,os,requests
url = "http://opendata.epa.gov.tw/ws/Data/REWXQA/?\
$orderby=SiteName&$skip=0&$top=1000&format=json"
# 讀取網頁原始碼
html=requests.get(url).text.encode('utf-8-sig')
# 判斷網頁是否更新
md5 = hashlib.md5(html).hexdigest()
if os.path.exists('old_md5.txt'):
    with open('old_md5.txt', 'r') as f:
        old_md5 = f.read()
    with open('old_md5.txt', 'w') as f:
        f.write(md5)
else:
    with open('old_md5.txt', 'w') as f:
        f.write(md5)
if md5 != old_md5:
    print('資料已更新...') 
else:
    print('資料未更新，從資料庫讀取...')





################################# numpy #################################

import numpy as np

##
data = np.array([[ 0.226, -0.23 , -0.86],[ 0.5639, 0.2379, 0.904]])
print(data)
print(data.ndim)
print(data.shape)
print(data.dtype)

## numpy slicing 
print(data[0:1])

## similar to range()
print(np.arange(10))
np.arange(0.1,0.5,0.1)  # we can use arange() for floats

## can multiply or divide
t1=np.array([1,2,3,4])
t2=t1*2
print(t2)

## reshape matrix
x=np.arange(4) 		// 0 1 2 3
print(x)
print(x.reshape(4,1))

## Transpose
x=np.zeros((10,3))  # a ndarray of 10 rows and 3 columns
print(x.T)
np.zeros(10)		// 0 0 0  for 10 zeros


## get random number
np.random.randn()     ## totally random float
np.random.randn(3,3)  ## a 3x3 matrix at random
a=np.random.randint(low=1,high=100,size=100)  ##random integer
print(a)
print(a.mean())
print(a.sum())
print(a.min())
print(a.max())
print(a.std())
print(a.cumsum())

## 排除numpy 遺漏值
__[~_____]


## correlation

np.corrcoef(x,y)
plt.scatter()

## 取log
np.log10()
np.log1p()  ## returns the ln of one plus input array


## concatenating two numpy arrays
x_train=np.concatenate((x_train_1,x_train_2),axis=0)

# finding the indexes of all True objects in a list
Method 1 . np.where(_list_) # this returns a nd.array

Method 2. Use itertools
from itertools import compress
compress(list1,list2)  # list1: items ; list2: boolean values ---> selects the truth items from list1



## using the any function
any() --= can input dictionary, list, strings
Return True if at least one element of an iterable is True, else False (if array is empty
or all elements are false)


# concatenate two ndarrays

np.concatenate((a,b), axis = 0 or 1)	// a, b are ndarrays

################################# pandas #################################


import pandas as pd

## pandas dataframe
## pandas dataframe的每一欄都是一個 pandas series
## pandas series 有含欄名，取出值使用__.values --> 這樣會變成numpy array (numpy.ndarray)


## pandas series : a 1 dimensional array that is labeled; has an index
print(cars['country'])  ## not using brackets

my_obj = pd.Series([4, 7, -5, 3],index=[])
my_obj.values  ## Dataframe / Series --> ndarray
my_obj.index
- 查詢index
_ in my_obj

## reindex
__.reindex(__a index__,method="fill")


## read files
- read csv
pd.read_csv()  ## read_csv(__.csv,index_col=0)  ## 代表用first column : column=0 為index ; 也可以specify column name

- read excel
pd.read_excel()


## read in date files pd.read_csv("__.csv",parse_dates=True,index_col="__")
pd.to_datetime(a_list,format="") ##'%Y-%m-%d %H:%M'


## 創建dataframe
bb=pd.DataFrame(aa,index=[22,33,44],columns=['one','two','three'],dtype=_'float'__)

## copy a dataframe/ or certain columns
cc=bb[['one','three']].copy()


##  Turn a dictionary to a pandas dataframe
scores = {"姓名":["小華","小明","小李"],
          "國文":[80,55,75],
          "數學":[90,70,45]}
score_df = pd.DataFrame.from_dict(scores)


# list of dictionaries to a pandas dataframe (row by row)
list_of_dicts = [
{'a':1,"b":2,"c":3},
{'a':4,"b":5,"c":6}
]
	a b c
0	1 2 3
1	4 5 6


## dictionary of lists to a pandas dataframe (column by column)
percentile_list = pd.DataFrame(
    {'lst1Title': lst1,
     'lst2Title': lst2,
     'lst3Title': lst3
    })
pd.DataFrame(data,columns=['','',....])



## checking basic information of dataframe
__.head()  ## 可以指定幾行 ex:head(10)
__.tail()
__.shape   ## returns data frame shape (row,col)
.size      ## returns rows
__.columns ## returns column names
.__index:
	- check index
	- specify index: ___.index = __somelist__
__.info()  ## check out name and data types of each column, also if exist any missing values
__.values  ## returns 2D numpy array
__.dtypes  ## show each column data type
__.describe() # only returns descriptions of continuous variables
__.dropna()


--------- Summary statistics --------- 
## count,mean,std,q1,q2,q3,max.min,quantile
__.count()  	# count
__.unique()
__.sum()	#can specify axis .sum(axis=0/1); axis=0, sum by rows. axis=1, sum by columns
__.mean()
__.median()
__.describe()
__.std()
__.quantile() ex: quantile(0.5) or quantile([0.5,0.75])
__.var()
__.std()
__.skew() 
	## skew = 0 perfectly Normal distribution
	## -0.5<skew<0.5 Normal
	## -1<skew<-0.5 ; 0.5<skew<1 moderately skewed
	## abs(skew)>1 highly skewed

__.kurt() ## kurtosis>3 --> heavier tails
__.corr() ## correlation matrix of all values * DataFrame.corr(method='pearson/kendall/spearman', min_periods=1) *
__.cov()
__.astype() 	# astype('int64')

__.cumsum()	# cumulative sum
__.cummax()	# cumulative maximum


** 相異值個別有多少個
pd.value_counts(__some column__)
pd.value_counts(__some column__,sort = True,normalize = True)	// Turns the counts to proportions



# transpose
_df_.T

## binning
bin_cut: a list of bin cut [10,15,20,...]
pd.cut(__column__, bins=bin_cut)  ## 等寬劃分 ; continuous variables的間距是不一樣的
pd.qcut(__column__, bins=bin_cut) ## 等頻劃分 : number amongst categories are the same



- Column Selection
# select two columns 
print(df[['Name', 'Qualification']])  ## get subdataframe

df['Name'] 相當於 df.Name


- Row selection (directly from data frame)
df[__:__]



# Column row subsetting

-loc 只能用在index「名稱」
bb.loc[[22,33]][['one','three']]   ## 前面先rows, 後面columns
score_df.loc["張小華",['國文','數學']] #選取張小華的國文與數學
score_df.loc[:,['國文']] #就是選所有人的國文分數

- iloc用在index「位置」
_mydataframe__.iloc[__,__]
EX:
_mydataframe__.iloc[:,:]

_mydataframe__.iloc[[1,2,3]] ## select 1,2,3 rows
_mydataframe__.iloc[[],[]]  ## 反正兩個以上row/column就要加[]
_mydataframe__.iloc[:,[]]

_mydataframe__.iloc[::3,]  ## 取every 3rd row


score_df.iloc['張小華'].idxmin() #張小華考最差的科目
score_df.iloc['張小華'].min() #張小華考最差的分數


- Column 條件式選取
** select dtypes
_df__.select_dtypes(include=['','',..])





**  filter()功能
gapminder[(gapminder['year'] == 2007) & (gapminder['continent'] == 'Asia')]]

df.column.isin([___,___,__...])  ## 欄位中包含... ex: dogs['___']isin([___,___])

** mutate()功能  / Use lambda functions
gapminder['country_abb'] = gapminder['country'].apply(lambda x: x[:3])

** summarize() 功能
----sum
gapminder[gapminder['year'] == 2007][['pop']].sum()
-mean
gapminder[gapminder['year'] == 2007][['lifeExp', 'gdpPercap']].mean()

** groupby
__df__.groupby(column)[select a columns].__a method__
dogs.groupby("color")['___'].mean()

# groupby combining with aggregate
__df__.groupby(_column__)[some column].agg([function1,function2,function3])

# groupby multiple columns
__df__.groupby(['',''])


Ex: Filter then groupby
gapminder[gapminder['year'] == 2007].groupby(by = 'continent')['pop'].sum()
gapminder[gap-minder['year'] == 2007].groupby(by = 'continent')[['lifeExp', 'gdpPercap']].mean()

df.groupby("Column").aggregate('count').reset_index()



** aggregate
df['column'].agg(function)

## Aggregate over specific columns
df[[__column1__,__column2__,__column3__]].agg(somefunction)
df[[__column1__,__column2__,__column3__]].agg(somefunction1,somefunction2)

## Aggregate multiple functions (sum, min) over the all columns.
>>> df.agg(['sum', 'min'])
        A     B     C

sum  12.0  15.0  18.0
min   1.0   2.0   3.0

## Different aggregations per column.
>>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})
        A    B
max   NaN  8.0
min   1.0  2.0
sum  12.0  NaN




** 判斷遺漏值
print(ironmen_df.ix[:, "groups"].isnull())
print(ironmen_df.ix[:, "ironmen"].notnull()) # 判斷哪些組的鐵人數不是遺失值



- Column Addition:
# Declare a list that is to be converted into a column 
address = ['Delhi', 'Bangalore', 'Chennai', 'Patna'] 
# Using 'Address' as the column name 
# and equating it to the list 
df['Address'] = address

** inplace - True : make changes in original data frame

- Column Deletion:
# dropping passed columns 
data.drop(["Team", "Weight"], axis = 1) # this will alsso do
data.drop(["Team", "Weight"], axis = 1, inplace = True) 
x= data.drop('somecolumn',axis=1 or 0).values   ## values turn pd dataframe to numpy array


- Row Selection:
Rows can also be selected by passing integer location to an iloc[] function.

# retrieving row by loc method 
first = data.loc["Avery Bradley"] 
second = data.loc["R.J. Hunter"]

# add rows
# simply concatenate both dataframes 
df = pd.concat([new_row, df]).reset_index(drop = True) 
df.head(5) 


#delete rows
# dropping passed values 
data.drop(["Avery Bradley", "John Holland", "R.J. Hunter"], inplace = True)
___.drop(column, axis =1)


- concatenation 
pd.concat([df1,df2]) # with rows
pd.concat([df1,df2],axis=1) # with columns

Outer Join 
pd.merge(df1,df2,on="id",how="outer") ## 遺失值以na補

Inner Join
pd.merge(df1,df2,on="id",how="inner") 

Left join 

pd.merge(df1,df2,on="__",how="left") 



- sort by column
__.sort_values(__column name___)
EX:   bb=bb.sort_values('one',ascending=False)
- sorting by multiple columns
__.sort_values([__column1__, __column2__])

** sort 1st column ascending, 2nd column descending
__.sort_values([__column1__, __column2__],ascending = [True,False])	


## getting a 2x2 table 
2by2table = pd.crosstab(df[col1],df[col2])

## pivot table (alternative to groupby)
Index = __	// group by this index
__df__.pivot_table(values = "__", index = "___",aggfunc = [function1, function2])
__df__.pivot_table(values = "__", index = "___",columns = some column, fill_value = 0, margins = True)	
// columns = ___ means we are further segmenting using the column

Ex:	# Print mean weekly_sales by department and type; fill missing values with 0
print(sales.pivot_table(values = "weekly_sales",index= ['type','department'],fill_value = 0 ))



## chi-square test using pandas columns
ex:
from scipy.stats import chi2_contingency

for c in data_mean.columns:
    if c != "Survived":
        two_by_two_table = pd.crosstab(data_mean[c],data_mean['Survived'])
        chi_square_results = chi2_contingency(two_by_two_table)
        print(f'The chi square value of {c} and Survived is {chi_square_results[0]}, leading\
        to a p-value of {chi_square_results[1]}, therefore we can conclude that the\
        the hypothesis that these two variables have a relationship is {chi_square_results[1]<0.05}\
        under 0.05 confidence. \n')

################################# Cleaning data in Python (DataCamp) #################################





**** Handling missing values ****

-- Nan

# check how many nas
__df__.isna().any() # check which columns have na
__df__isna().sum()  # check which columns have how many nas

# fill nas
__df__.fillna("None") # fill na with string "None"

# drop nas
__df__.dropna()  # drop all na values


# drop duplicates
df.drop_duplicates()
Ex:
df.drop_duplicates(subset = [__column1__,__column2__])	//dropping duplicates based on 2 columns

# sampling
df.sample(frac=0.5)  ## random 50 % data
df.sample(n=10) 


https://oranwind.org/python-pandas-ji-chu-jiao-xue/
http://codingpy.com/article/a-quick-intro-to-pandas/
https://morvanzhou.github.io/tutorials/data-manipulation/np-pd/3-1-pd-intro/


__.clip()
EX:
# 將 GrLivArea 限制在 800 到 2500 以內, 調整離群值
df['GrLivArea'] = df['GrLivArea'].clip(800, 2500)








################################# Plotting with pandas data frame #################################

https://medium.com/datainpoint/從-pandas-開始-python-與資料科學之旅-8dee36796d4a
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html

--- Requirements ---
import matplotlib.pyplot as plt
import seaborn as sns  		// builds on top of Matplotlib

* Saving plots
plt.savefig('output.png')	# the plot gets saved to 'output.png'

* Show the plot
plt.show()

* Configure the plot
plt.figure(figsize = (15,12))
plt.xlabel("____")
plt.ylabel("____")
plt.title(" ")
plt.subplots(nrows=3, ncols=1)
plt.legend()	# ex: plt.legend([__,__])




kind — ‘bar’,’barh’,’pie’,’scatter’,’kde’ etc which can be found in the docs.
color — Which accepts and array of hex codes corresponding sequential to each data series / column.
linestyle — ‘solid’, ‘dotted’, ‘dashed’ (applies to line graphs only)
xlim, ylim — specify a tuple (lower limit, upper limit) for which the plot will be drawn
legend— a boolean value to display or hide the legend
labels — a list corresponding to the number of columns in the dataframe, a descriptive name can be provided here for the legend
title — The string title of the plot
rot - Rotation degree
alpha - visibility, 0:transparent ~ 1:opaque(full)

* Scatter plot
__df__.plot(kind='scatter',x='',y='',color='')
plt.show()

* Bar plot
__df__.plot(kind='bar',x='',y='')


- Barplot with groupby
df.groupby('state')['name'].unique().plot(kind='bar')
plt.show()


* Line plot
(Multiple columns)
ax = plt.gca()
__df__.plot(kind='line',x='',y='',ax=ax)			// first line
__df__.plot(kind='line',x='',y='', color='', ax=ax)		// second line
plt.show()
__df__.plot(x=__,y=__,kind = "line",rot = 45)	// rotate x ticks by 45 deg
 

* Histogram
__df__.hist(column='', bins=50)
__df__[__some column__].hist()	// let histogram be more transparent: alpha = 0.7

# creating multiple histograms simultaneously
__df__[[column1,column2,...]].hist()



* piechart
__df__.plot.pie(y='some_column',figsize=(5, 5),autopct='%1.1f%%', startangle=90)
plt.show()



################################# Data visualization with Seaborn (Pandas + Matplotlib) #################################

* Pairplots for column to column
sns.pairplot(car_data.loc[:,car_data.dtypes == 'float64'])









################################# regex 正則表達式 #################################
import re




################################# random #################################
import random

print( random.randint(1,10) )        # 产生 1 到 10 的一个整数型随机数  
print( random.random() )             # 产生 0 到 1 之间的随机浮点数
print( random.uniform(1.1,5.4) )     # 产生  1.1 到 5.4 之间的随机浮点数，区间可以不是整数
print( random.choice('tomorrow') )   # 从序列中随机选取一个元素
print( random.randrange(1,100,2) )   # 生成从1到100的间隔为2的随机整数
print(random.sample(a,2))  ## get two samples from list or set a

a=[1,3,5,6,7]                # 将序列a中的元素顺序打乱
random.shuffle(a)
print(a)

import numpy as np
np.random.rand()   create number from 0~1
np.random.seed()
################################# Django ###########################################
django-admin --version  ## currently 2.2.1

** activating virtual environment
 . bin/activate 
** 建 project!
django-admin startproject xvapp 
** run project  (要切換至project的根目錄)
python manage.py runserver
** 建app：就是一個功能
python manage.py startapp [app_name]


admin.py : 設定資料庫呈現的模式，之後會跟models溝通
models.py : 建構你的資料庫型態
tests.py : 這是拿來檢查商業邏輯的地方，也就是用來測試你的邏輯是否有遺漏，這裡我們沒有要討論太多這方面的議題，但是你要記住，寫測試是一件相當重要的事情，千萬不能小看使用者的潛能
views.py : 相信你還沒有忘記，在 Day1我們這一位擔任控制者(controller)的身分，沒錯! 它就是寫商業邏輯的地方，它會跟urls.py做呼應，並將所需傳達給前端
urls.py : 它擔任著橋樑的角色，讓views.py與相對的網站做對應。蛤? 你說怎麼沒見到 urls.py，因為我們要自己建阿^^，在這裡建議可以把內部 ithome的urls.py複製貼過來就好，但是內容要修改!! 待會再來說明修改的部分
apps.py : 這裡你只要先了解，這是用來區別app的一個檔案即可
init.py : 相信大家都還沒有忘記^^，就是告訴Python這資料夾是個套件
migrations : 這資料夾裡面存放的內容，記錄著models裡面所創建的資料庫型態，這部分後面會詳談

################################# Importing #################################

Two kinds of import.

*
from sklearn import linear_model
linear_model.LinearRegression()

*
from sklearn.linear_model import LinearRegression

*
Import from another file from the same directory


################################# Scikit learn ###########################################



################################# Feature Engineering ###########################################
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler


### Standard encoding 將連續變項標準化
#标准化，返回值为标准化后的数据
StandardScaler().fit_transform(data)


### Label encoding (used in ml model)
- converts each value to a number ; might lead algorithm to think the data is ordinal when it isn't

LEncoder = LabelEncoder()
LEncoder.fit_transform(df[some_column])


### Min max scaler disregards Nans ;and retains after transformation (將區間縮放到0~1)
MMEncoder = MinMaxscaler()
df_mm = MMEncoder.fit_transform(df) #fit then transform



---- sample code ------
LEncoder = LabelEncoder()
MMEncoder = MinMaxScaler()
for c in df.columns:
    df[c] = df[c].fillna(-1)
    if df[c].dtype == 'object':
        df[c] = LEncoder.fit_transform(list(df[c].values))
    df[c] = MMEncoder.fit_transform(df[c].values.reshape(-1, 1))
df.head()

--------------------------


### Boxcox
from scipy.stats import boxcox
boxcox(x)



### One Hot encoding (used in dl models) // only takes in numerical values

1. pd.get_dummies(column or entire data frame)	// more flexible

2. 
from sklearn.preprocessing import OneHotEncoder
#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据
- OneHotEncoder().fit_transform(iris.target.reshape((-1,1)))
- pd.DataFrame(OneHotEncoder().fit_transform(df[some_column]).toarray())




### Mean encoding
for col in df.columns:
    if col != "Name":
        mean_col = data_mean.groupby([col])['Targeted_Y'].mean().reset_index()  ## get mean value of different categories to represent that category
        mean_col.columns = [col,str(col)+"_mean"]
        data_mean = pd.merge(data_mean,mean_col,on=col, how="left") ## left merge to original df
        data_mean = data_mean.drop([col],axis =1)  ## drop original column

data_mean = data_mean.drop(['Survived'], axis =1)



# Binarizer 將連續變項二元化
from sklearn.preprocessing import Binarizer

#二值化，阈值设置为3，返回值为二值化后的数据
Binarizer(threshold=3).fit_transform(iris.data)






############# Feature selection #############

1, Using L1(Lasso regression):
Reduce unimportant variables to 0 when lambda becomes large
Lasso regression loss function : ∑(i from 1 to n)(Yi - ∑(j=1 to p) xijβj)^2 + ƛ∑(j=1 to p)|βj|

from sklearn.linear_model import Lasso
lasso = Lasso(alpha=.3)
lasso.fit(X, Y)


2. L2 ridge regression
∑(i from 1 to n)(Yi - ∑(j=1 to p) xijβj)^2 + ƛ∑(j=1 to p)βj^2
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=10)
    ridge.fit(X,Y)
    print("Ridge model:", pretty_print_linear(ridge.coef_))



################# Leaf Encoding : Day 32 ML100 #################
Method:
use gradient boosting classifier or random forest classifier to fit trainX to get categories
These categories are better than human discriminated categories, then we use one hot encoder
To encode, and fit logistic regression or factorization machine (?) to get new predictions.


---
Training set
Testing set
Validation set










################################# Regression #############################################

## Linear regression
from sklearn.linear_model import LinearRegression
estimator = LinearRegression()
estimator.fit(train_X, train_Y)
# 可以接cv
pred = estimator.predict(test_X)



















## Logistic regression	// can be used in binary or multhclass classification

from sklearn.linear_model import LogisticRegression
estimator = LogisticRegression()
estimator.fit(train_X, train_Y)
pred = estimator.predict(test_X)

When to use:
1. When target variable is binary
2. Get probability results
3. When data is linearly separable / when you need to understand the impact of a feature
4. If we want to understand the impact of a feature

Method:

Sigmoid function: (logarithm function)

ΘTX : Θtranspose X
σ(ΘTX) =  1/(1+e^(-ΘTX))

If ΘTX is big, then it becomes 1
If ΘTX is small, then it becomes 0

Output:
p(Y=1 | X) 
p(Y=0 | X) = 1 -  p(Y=1 | X) 

Cost Function: compare yhat with y 
For instance, if yhat(The probability of the estimated y) is 0.8, and y actual is 1, then the error is 0.2

Training process:

How to set the parameters or change parameters?
Gradient descent using derivatives

### look up


## Random Forest regressor
from sklearn.ensemble import RandomForestRegressor



#### Train test split and Cross Validation. (Day 34 Ml100)

https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation


## Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.4, random_state=0)

- random state: seed for reproducibility


Ex:
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25)	// stratify based on y 



## Cross Validation
https://scikit-learn.org/stable/modules/cross_validation.html

from sklearn.model_selection import cross_val_score
scores = cross_val_score(_model__,train_x,train_y,cv=?) ## check how many cross validations to do
scores



K Fold cross validation

# Training sets, validation sets, and testing sets
Training set consists: Training set and validation set
Testing set: Used for testing the model trained on the training set

from sklearn.model_selection import KFold

kf = KFold(n_splits=5)
i = 0
for train_index, test_index in kf.split(X):
    i +=1 
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print("FOLD {}: ".format(i))
    print("X_test: ", X_test)
    print("Y_test: ", y_test)
    print("-"*30)



## Multi-label classification
## text classification example
https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff


#### Ensemble Learning Notes #### 
利用給予不同演算法權重，使其投票，做出預測，通常可以比單一演算法，或得更好的結果（預測率）



## Boosting (state of art machine learning algo to deal with structured data)
What is boosting?
Combination of weak learners into a strong learner, for all t, based on previous outcome t-1. We
give lower weight to the correct predictions, and higher weight to miss-classified ones

** gradient boosting regressor
from sklearn.ensemble import GradientBoostingRegressor
estimator = GradientBoostingRegressor()

** XGboosting (Extreme gradient boosting)
Base learners: Trees (cART:  classification and regression trees)

https://www.datacamp.com/community/tutorials/xgboost-in-python#what



## reaching the optimal set of hyperparameters in machine learning

Grid Search, Random Search and Bayesian Optimization





################################# Classification #################################
Is Supervised Learning.
Use labeled data to predict the category of a new row of data 


---------  K Nearest Neighbors ---------
binary classification or multi-class classification / can also be used in regression


Methods:
1. Choose k nearest neighbors and see the majority of categories (ie. Which category has the most proportion)
to decide what category to assign

2. Classifying cases based on their similarity (e.g. Euclidean distance ) to other cases

* How to choose the right k?
	Do cross validation/ fit on test data :  to search for highest accuracy model to determine k

KNN for continuous target variables:
Takes median/ mean for the neighbors to predict the target variable

Sample Code:

from sklearn.neighbors import KNeighborsClassifier

k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
neigh

yhat = neigh.predict(X_test)
yhat[0:5]


---------------  Decision Trees -----------------
Building decision trees:	(Recursive partitioning)

Methods:
How to determine which attribute is the best?
Calculate prevalence for every attribute in the categories with groupby
High Predictiveness: decrease impurity, lower entropy

-- Calculating entropy: amount of randomness	(we want less entropy)
The lower the entropy, the higher the purity, and the less uniform distribution

e.g.
For one node, we want to calculate for category A and category B
If, category A and B are 0.5 and 0.5 in terms of proportion, then entropy is 1, this variable is no use
If category A is 1, which is 100% in the node, or vice versa, then entropy is 0

Entropy formula: p(A)log(p(A)) - p(B)log(p(B))
p(A) : the probability of A amongst all data


-- Information gain:  (we want more information gain)
Information gain = Entropy before split - weighted entropy after split
eg.
Originally: 9B5A
Entropy before split: 0.94

After first node split:
F: 3B4A entropy 0.985
M: 6B1A entropy 0.592

weighted entropy after split:(7/14)*0.985 + (7/14)*0.592


Information gain = 0.94 - (7/14)*0.985 + (7/14)*0.592)

---------------  Support Vector machine (svm) -----------------

Mapping data to a higher dimensional space: 
- Linear
- Polynomial
- Rbf (radio basis function)
- Sigmoid

How to find the hyperplane?

Largest separation / highest margin between two classes

Examples closest two the hyperplane are the support vectors, and only them matter
We try to choose the hyperplane with max distance to support vectors

Disadvantages:
Prone to overfitting if feature numbers > n
Not computationally efficient, n > 1000

Applications:
Image recognition
Text mining (detect spam, sentimental analysis)






---------------  Gradient Boosting Classifier -----------------

from sklearn.ensemble import GradientBoostingClassifier
# 因為擬合(fit)與編碼(transform)需要分開, 因此不使用.get_dummy, 而採用 sklearn 的 OneHotEncoder





---------------  Random Forest Classifier -----------------
from sklearn.ensemble import RandomForestClassifier






################################# Metrics evaluation ###############

---------  Evaluation Metrics in Classification ---------

*** Jaccard Index : the union between predicted and actual labels

*** F-1 score: 

Precision: TP/(TP+FP)	// TP: true positive  模型判定瑕疵，樣本確實為瑕疵得比例
Recall: TP/(TP+FN)      模型判定瑕疵佔所有瑕疵的比例
F1 score = (2*(precision*recall)) / (precision+recall)


(Model)	\ (Reality)	Positive     Negative

Positive		TP		FP
 

Negative		FN		TN





Best: 1	// when precision is 1 and recall is 1
Worst: 0

Classifier has good recall and good precision  ----> good classifier


--- code -----
Precision, recall, and F1 score

y_pred_binarized: probability transformed to categorical variables depending on threshold (y_pred_binarized = np.where(y_pred>threshold, 1, 0) )

f1 = metrics.f1_score(y_test, y_pred_binarized) # 使用 F1-Score 評估
precision = metrics.precision_score(y_test, y_pred_binarized) # 使用 Precision 評估
recall  = metrics.recall_score(y_test, y_pred_binarized) # 使用 recall 評估















*** Log loss
Measures the performance of the predicted output
logloss = -(1/n)∑(y*log(yhat) +(1-y)*log(1-yhat))

Ideal classifiers have low log loss.

Sample code:
from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))


** ROC curve

from sklearn.metrics import roc_curve
y_pred: needs to be probability



** AUC score
metrics.roc_auc_score(y_test, y_pred) 	// y_pred needs to be probability




** Top K accuracy


---------  Evaluation Metrics in regression ---------

from sklearn import metrics

** Mean absolute error

metrics.mean_absolute_error(prediction, y) 

** Mean squared error
metrics.mean_squared_error(prediction, y)

** R square
metrics.r2_score(prediction, y)






################################# Clustering #################################

---------------  K means clustering (only suitable for continuous variables, use K-modes for categorical variables)-----------------
Method:
1. Random initialized k centroids
2. Get distance matrix (euclidean distance), and assign each data point to a cluster
3. Calculate the center of each cluster, and make it the new centroid
4. Iterate until convergence (def: minimizing the intra-cluster distance, and maximize the inter-cluster distance)


***** Step into clustering *****

1. Normalize continuous data (e.g. Standard Scaler)


*********** Code ************
from sklearn.cluster import KMeans 
k_means = KMeans(init = "k-means++", n_clusters = 4, n_init = 12) 	

// n_init: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. (Inertia: The coordinates remain the same)

k_means.fit(X)
k_means_labels = k_means.labels_
k_means_cluster_centers = k_means.cluster_centers_

---------------  Hierarchical clustering (Agglomerative: bottom up) ---------------
Methods:
1. Create n clusters, one for each data point
2. Compute proximity matrix (distance: Euclidean distance)
3. Merge two closest clusters and update the proximity matrix
4. Repeat 2,3,4 until a single cluster remains


How to calculate distance between clusters?
- Single linkage clustering : min distance between clusters
- Complete Linkage clustering : max distance between clusters
- Average Linkage clustering : Mean distance of the distance of every point in one cluster to every point in another cluster
- Centroid linkage clustering : distance of centroid of a cluster to centroid of another

Pros:
Does not require specifying the number of clusters (eg: K)
Always generates the same clusters

Cons:
Long run time

*********** Code ************
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster import hierarchy 

dist_matrix = distance_matrix(feature_mtx,feature_mtx) 
print(dist_matrix)
agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')
agglom.fit(feature_mtx)
agglom.labels_
pdf['cluster_'] = agglom.labels_
pdf.head()	//pdf is the original dataset





## plotting
import matplotlib.cm as cm
n_clusters = max(agglom.labels_)+1
colors = cm.rainbow(np.linspace(0, 1, n_clusters))
cluster_labels = list(range(0, n_clusters))

# Create a figure of size 6 inches by 4 inches.
plt.figure(figsize=(16,14))

for color, label in zip(colors, cluster_labels):
    subset = pdf[pdf.cluster_ == label]
    for i in subset.index:
            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) 
    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)
#    plt.scatter(subset.horsepow, subset.mpg)
plt.legend()
plt.title('Clusters')
plt.xlabel('horsepow')
plt.ylabel('mpg')




---------------  DBSCAN (Density Based Clustering) ---------------

can detect outliers
Two parameters : R; radius, M: minimum points

Three types of data points: 
Core: has m points in R
Border: neighborhood has less than M points or is reachable by a core point
Outlier: not core or border point

Connect core points that are neighbors, and assign as the same cluster

Pros:
Doesn't require specification of K

*********** Code ************
??





################################# urllib #################################

 模組	 說明
 urllib.request	 	開啟並讀取 URL
 urllib.error	 	捕捉 urllib.requests 引起之例外
 urllib.parse	 	剖析 URL
 urllib.robotparser	剖析 robot.txt 檔











################################# LINEBOT #################################
## 部署 Heroku，並將files push 至 git 
heroku login -i
git config --global user.name "你的名字"
git config --global user.email 你的信箱

## https://ithelp.ithome.com.tw/articles/10217350

##  Git push 
- get into your file path (因為我想要這個file推向Heroku)
- git init
- heroku git:remote -a llbot1
- git add .
- git commit -m "test"
- git push heroku master (push to master)


## check logs
heroku logs

# event 內含
event = {"reply_token":"就是代表reply_token的一串亂碼", 
         "type":"message",
         "timestamp":"1462629479859", 
         "source":{"type":"user",
                   "user_id":"就是代表user的一串亂碼"}, 
         "message":{"id":"就是代表這次message的一串代碼", 
                    "type":"text", 
                    "text":"使用者傳來的文字信息內容"}}

################################# Unit testing #################################


do with functions in python 
Test each branch (including every input)

Comparing float values: (can differ in subtle floats values)
From pytest import approx

approx(function() == ___) ##replacing assert in pytest



## Prompt input in the main function








################################# Crontabs (Automating work) #################################

#crontab guru  # check out the correct time to execute script



# view cronjobs 
#crontab -l

# add a new cronjob 
#crontab -e



# Minute Hour Day Month Day_of_the_Week

# ┌───────────── minute (0 - 59)
# │ ┌───────────── hour (0 - 23) 
# │ │ ┌───────────── day of month (1 - 31)
# │ │ │ ┌───────────── month (1 - 12)
# │ │ │ │ ┌───────────── day of week (0 - 6) (Sunday to Saturday;
# │ │ │ │ │                                       7 is also Sunday on some systems)
# │ │ │ │ │
# │ │ │ │ │
# * * * * *  command to execute


# which python
#/Library/Frameworks/Python.framework/Versions/3.8/bin/python3



## write python script 
(Add to beginning of file)
#!/usr/bin/env python    

# give access to the script
chmod +x __the script__


## can do multiple commands in crontab
'';
MAILTO=your.email@email.com
* * * * * cd /Users/user.name/dataanalysis/test && /usr/local/bin/python /Users/user.name/dataanalysis/test/py_script.py
'''
#50 19 * * * python hello.py >> a.txt

# using python in writing crontab
#https://stackabuse.com/scheduling-jobs-with-python-crontab/



## don't want to use cronjob
Simply comment it by #




-----------------
# killing a cronjob
#pkill process-name
#kill pid
# find pid of a program
ps ax | grep test.py






------- Future projects -----

Sentimental analysis on stock

The most common sentiment indicators are:

Put/Call Ratio
Volatility Index
Client Sentiment










